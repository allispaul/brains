{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a0ab9f-8417-442a-95f9-9b709cc94a9b",
   "metadata": {},
   "source": [
    "# KL Divergence\n",
    "\n",
    "In this notebook I look at Kaggle's KL divergence metric and some alternate ways of calculating it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c8f3d-90c2-41ec-8886-bc5fcc73006f",
   "metadata": {},
   "source": [
    "## Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aa71df4-748f-489b-9c48-db4e7c3ad83a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3bec5484-a04e-4493-9a24-41a9ba95260b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is Kaggle's function, see https://www.kaggle.com/code/metric/kullback-leibler-divergence/notebook\n",
    "# AFAICT the arguments `micro_average` and `sample_weights` are not used for this competition.\n",
    "def kl_divergence(solution: pd.DataFrame, submission: pd.DataFrame, epsilon: float, micro_average: bool, sample_weights: Optional[pd.Series]):\n",
    "    # Overwrite solution for convenience\n",
    "    for col in solution.columns:\n",
    "        # Prevent issue with populating int columns with floats\n",
    "        if not pandas.api.types.is_float_dtype(solution[col]):\n",
    "            solution[col] = solution[col].astype(float)\n",
    "\n",
    "        # Clip both the min and max following Kaggle conventions for related metrics like log loss\n",
    "        # Clipping the max avoids cases where the loss would be infinite or undefined, clipping the min\n",
    "        # prevents users from playing games with the 20th decimal place of predictions.\n",
    "        submission[col] = np.clip(submission[col], epsilon, 1 - epsilon)\n",
    "\n",
    "        y_nonzero_indices = solution[col] != 0\n",
    "        solution[col] = solution[col].astype(float)\n",
    "        solution.loc[y_nonzero_indices, col] = solution.loc[y_nonzero_indices, col] * np.log(solution.loc[y_nonzero_indices, col] / submission.loc[y_nonzero_indices, col])\n",
    "        # Set the loss equal to zero where y_true equals zero following the scipy convention:\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.rel_entr.html#scipy.special.rel_entr\n",
    "        solution.loc[~y_nonzero_indices, col] = 0\n",
    "\n",
    "    if micro_average:\n",
    "        return np.average(solution.sum(axis=1), weights=sample_weights)\n",
    "    else:\n",
    "        return np.average(solution.mean())\n",
    "\n",
    "# Simplified version\n",
    "def kl_div_simple(solution: np.ndarray, submission: np.ndarray, epsilon: float):\n",
    "    return kl_divergence(pd.DataFrame({\"votes\": solution}),\n",
    "                         pd.DataFrame({\"votes\": submission}),\n",
    "                         epsilon, False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bc0527de-3728-4beb-933f-ef52f3c9c062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: [4 0 0 2 0 0]\n",
      "Say our model predicts: [0.91 0.05 0.04 0.   0.   0.  ]\n",
      "First, clip everything to be strictly between 0 and 1.\n",
      "[0.91  0.05  0.04  0.001 0.001 0.001]\n",
      "Then, calculate P * log(P/Q), where P = target, Q = submission.\n",
      "This is set to 0 when P = 0.\n",
      "[ 5.92242016  0.          0.         15.20180492  0.          0.        ]\n",
      "The KL divergence is the mean:\n",
      "3.5207041802414487\n",
      "3.5207041802414487\n"
     ]
    }
   ],
   "source": [
    "# Example of usage\n",
    "target = np.array([4, 0, 0, 2, 0, 0])\n",
    "preds = np.array([0.91, 0.05, 0.04, 0, 0, 0])\n",
    "print(\"Target:\", target)\n",
    "print(\"Say our model predicts:\", preds)\n",
    "print(\"First, clip everything to be strictly between 0 and 1.\")\n",
    "eps = 0.001\n",
    "preds_clipped = np.clip(preds, eps, 1-eps)\n",
    "print(preds_clipped)\n",
    "print(\"Then, calculate P * log(P/Q), where P = target, Q = submission.\")\n",
    "print(\"This is set to 0 when P = 0.\")\n",
    "kl_arr = np.array([P*np.log(P/Q) if P != 0 else 0 for P, Q in zip(target, preds_clipped)])\n",
    "print(kl_arr)\n",
    "print(\"The KL divergence is the mean:\")\n",
    "print(kl_arr.mean())\n",
    "print(kl_div_simple(target, preds, eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48f2d4-296d-47c7-9cc3-3188f8e33833",
   "metadata": {},
   "source": [
    "## SciPy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39f6201d-88f2-4582-a221-31d7bde63b60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.731940219993192"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "# Again, we have to clip to get a finite result\n",
    "entropy(target, np.clip(preds, eps, 1-eps))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452de7e-a0a5-460b-856d-1428c07f8072",
   "metadata": {},
   "source": [
    "This differs from the Kaggle function in two ways:\n",
    "1. SciPy normalizes both inputs to sum to 1.\n",
    "2. SciPy takes the sum, rather than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "699b5a31-f821-4a90-8b8f-11528b7f4a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With target normalized:\n",
      "0.2881574518355656\n",
      "Summed rather than meaned:\n",
      "1.7289447110133938\n"
     ]
    }
   ],
   "source": [
    "target_norm = target / np.sum(target)\n",
    "print(\"With target normalized:\")\n",
    "print(kl_div_simple(target_norm, preds, eps))\n",
    "print(\"Summed rather than meaned:\")\n",
    "print(kl_div_simple(target_norm, preds, eps) * len(target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2dd8b-1617-4e67-9f54-bab1d93a72c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "SciPy's is slightly inaccurate -- not sure why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7a9e8-8838-42f9-a4c8-009f457cc69c",
   "metadata": {},
   "source": [
    "## PyTorch version\n",
    "\n",
    "This expects `preds` to be log-probabilities. NOT logits, as I'd said earlier! Logits are $\\log \\frac{p}{1-p}$. Here we just want $\\log p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4247127f-c8de-4657-90b2-dc4d2dfb2c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pvk/miniconda3/envs/brains/lib/python3.12/site-packages/torch/nn/functional.py:2949: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.5207041802414487"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import KLDivLoss\n",
    "loss_fn = KLDivLoss()\n",
    "preds_log = np.log(preds_clipped)\n",
    "preds_tensor = torch.tensor(preds_log)\n",
    "targ_tensor = torch.tensor(target, dtype=torch.float64)  # Throws an error if handed ints\n",
    "loss_fn(preds_tensor, targ_tensor).item()  # Note the argument order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2c6de135-491a-4f52-8bcc-e4651d366b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0943, -2.9957, -3.2189, -6.9078, -6.9078, -6.9078],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f99d06a-142d-4da5-aea2-cb9e67058596",
   "metadata": {},
   "source": [
    "If our model output is `preds_tensor`, then we're expecting `exp(preds_tensor)` to be our vector of probabilities. This means that it's not meaningful for `preds_tensor` to have any nonnegative entries. If our final model layer is linear, I guess we should pass its output through an activation function that restricts it to the negative reals. It seems like a natural choice would be $\\log \\circ\\, \\mathrm{softmax}$ -- this would correspond to regarding our model outputs as logits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c01b9-2fa1-40d3-aeb1-6c3339739a85",
   "metadata": {},
   "source": [
    "A further complication with the PyTorch version is its behavior on batches. The default behavior (which the documentation calls incorrect) is to take the mean over the whole batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97121083-aa98-4d78-8831-6f46b1e295bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5207, dtype=torch.float64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_batch = torch.stack([targ_tensor for _ in range(32)])\n",
    "preds_batch = torch.stack([preds_tensor for _ in range(32)])\n",
    "loss_fn(preds_batch, targ_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6373f5-f1f0-4e01-bb5f-d71d08cdeac4",
   "metadata": {},
   "source": [
    "The \"correct\" behavior is to just take the mean in the batch direction. This gives a result which is `len(target)` times as large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "13fd5860-4411-4a94-80b8-8ef2c31e8317",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.1242, dtype=torch.float64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn_batchmean = KLDivLoss(reduction=\"batchmean\")\n",
    "loss_fn_batchmean(preds_batch, targ_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfe78c-6c03-46f2-87b3-5ca2ab3ad2cb",
   "metadata": {},
   "source": [
    "This discrepancy didn't appear in our first PyTorch example because we passed in 1D tensors. But if we do 2D tensors with a batch size of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6ee8e373-a3c2-4fdd-854f-a8f30aa7224c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.1242, dtype=torch.float64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn_batchmean(preds_tensor.unsqueeze(0), targ_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccacf7c6-bea5-46fe-9f53-f26352db908f",
   "metadata": {},
   "source": [
    "But what does Kaggle do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1fbd7f28-f70c-4efc-90cc-7e62311b201d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5207041802414487"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_divergence(pd.DataFrame(targ_batch), pd.DataFrame(preds_batch.exp()), eps, False, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a113e8-e87b-4d41-a135-9aca96772093",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "- We should use `torch.nn.KLDivLoss(reduction=\"mean\")` and ignore the warning. This will calculate roughly the same thing as Kaggle, up to clipping.\n",
    "- Model outputs should be constrained to be negative. I suggest a final activation function of $\\log \\circ\\, \\mathrm{softmax}, which keeps the final linear layer outputs interpretable as logits.\n",
    "- The clipping in the Kaggle algorithm effectively forbids us from predicting a probability that's too low. Practically speaking, I think we don't need to use any corresponding clipping during training: A very small predicted probability will either result in a high loss penalty (if the target distribution has a nonzero probability there), or not contribute to either Kaggle's or our loss at all (if the target distribution is 0 there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5485bc-d08a-48a6-b3a0-1459586ad9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brains",
   "language": "python",
   "name": "brains"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
