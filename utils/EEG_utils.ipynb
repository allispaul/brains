{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6946c0-a5c1-44e3-af96-0c06a8b4f87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\EEG\\all_brains\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'all_brains'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/allispaul/brains.git all_brains\n",
    "%cd all_brains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8016eb-0466-4dff-991e-d3e3fd095208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09ba0b-32fc-4b59-aacc-e23477864630",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From training.py ###\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Callable\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "MODEL_SAVE_DIR = Path(\"models\")\n",
    "\n",
    "\n",
    "def cycle(iterable):\n",
    "    # from https://github.com/pytorch/pytorch/issues/23900#issuecomment-518858050\n",
    "    iterator = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(iterable)\n",
    "\n",
    "class Trainer():\n",
    "    \"\"\"Bundles together a model, a training and optionally a validation dataset,\n",
    "    an optimizer, and loss/accuracy/fbeta@0.5 metrics. Stores histories of the\n",
    "    metrics for visualization or comparison. Optionally writes metric hsitories\n",
    "    to TensorBoard.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 train_loader: data.DataLoader,\n",
    "                 val_loader: data.DataLoader,\n",
    "                 *,\n",
    "                 optimizer: type[torch.optim.Optimizer] = optim.SGD,\n",
    "                 criterion: nn.Module = nn.KLDivLoss(),\n",
    "                 lr: float = 0.03,\n",
    "                 scheduler: Optional[type[torch.optim.lr_scheduler]] = None,\n",
    "                 writer: SummaryWriter | str | None = None,\n",
    "                 model_name: str | None = None,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        \"\"\"Create a Trainer.\n",
    "        \n",
    "        Args:\n",
    "          model: Model to train.\n",
    "          train_loader: DataLoader containing training data.\n",
    "          val_loader: DataLoader containing validation data.\n",
    "          optimizer: Optimizer to use during training; give it a class, not an\n",
    "            instance (SGD, not SGD()). Default torch.optim.SGD.\n",
    "          criterion: Loss criterion to minimize. Default nn.KLDivLoss().\n",
    "          lr: Learning rate. Default 0.03.\n",
    "          scheduler: Optional learning rate scheduler. As with optimizer, give\n",
    "            a class, which will be initialized at the start of training. If no\n",
    "            scheduler is given, a constant learning rate is used.\n",
    "          writer: SummaryWriter object to log performance to TensorBoard. You\n",
    "            can create this using .logging.create_writer(). If writer is\n",
    "            \"auto\", a SummaryWriter will automatically be created, using\n",
    "            model_name (which is required in this case).\n",
    "          model_name: Name of the model. Will be used to save checkpoints for\n",
    "            the model and to automatically create a SummaryWriter.\n",
    "          Keyword arguments prefixed by `optimizer_` or `scheduler_` are passed\n",
    "            to the optimizer or scheduler, respectively.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.lr = lr\n",
    "        self.scheduler_class = scheduler\n",
    "        self.optimizer_kwargs = dict()\n",
    "        self.scheduler_kwargs = dict()\n",
    "        self.scheduler_kwargs.setdefault('max_lr', self.lr)\n",
    "        for key in kwargs.keys():\n",
    "            # strip off optimizer or scheduler prefix and add to relevant dict\n",
    "            if key.startswith('optimizer_'):\n",
    "                self.optimizer_kwargs.update({key[10:]: kwargs[key]})\n",
    "            if key.startswith('scheduler_'):\n",
    "                self.scheduler_kwargs.update({key[10:]: kwargs[key]})\n",
    "        self.optimizer = optimizer(model.parameters(), lr=self.lr,\n",
    "                                   **self.optimizer_kwargs)\n",
    "        if self.scheduler_class is not None:\n",
    "            self.scheduler = self.scheduler_class(\n",
    "                self.optimizer,\n",
    "                **self.scheduler_kwargs\n",
    "            )\n",
    "            \n",
    "        self.histories = {\n",
    "            'epochs': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'lr': []\n",
    "        }\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.writer = writer\n",
    "        if self.writer == \"auto\":\n",
    "            if model_name is None:\n",
    "                raise ValueError('model_name is required with writer=\"auto\"')\n",
    "            self.writer = create_writer(model_name)\n",
    "            \n",
    "    def get_last_lr(self) -> float:\n",
    "        \"\"\"Get the last used learning rate.\n",
    "        \n",
    "        Looks in the scheduler if one is defined, and if not, looks in the\n",
    "        optimizer. Assumes that a single learning rate is defined for all\n",
    "        parameters.\n",
    "        \n",
    "        Returns:\n",
    "          The last used learning rate of the trainer.\n",
    "        \"\"\"\n",
    "        if self.scheduler_class is not None:\n",
    "            return self.scheduler.get_last_lr()[0]\n",
    "        else:\n",
    "            return self.optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    def train_step(self, X, y):\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(X.to(DEVICE))\n",
    "        loss = self.criterion(outputs, y.to(DEVICE))\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if self.scheduler_class is not None:\n",
    "            self.scheduler.step()\n",
    "        return outputs, loss\n",
    "            \n",
    "    def val_step(self, X, y):\n",
    "        with torch.inference_mode():\n",
    "            val_outputs = self.model(X.to(DEVICE))\n",
    "            val_loss = self.criterion(val_outputs, y.to(DEVICE))\n",
    "        return val_outputs, val_loss\n",
    "    \n",
    "    def train_eval_loop(self, epochs, val_epochs, val_period: int = 500,\n",
    "                        save_period: int | None = None):\n",
    "        \"\"\"Train model for a given number of epochs, performing validation\n",
    "        periodically.\n",
    "        \n",
    "        Train the model on a number of training batches given by epochs. Every\n",
    "        val_period training batches, pause training and perform validation on\n",
    "        val_epochs batches from the validation set. Each time validation is\n",
    "        performed, the model's loss, accuracy, and F0.5 scores are saved to the\n",
    "        trainer, and optionally written to TensorBoard. Optionally, periodically\n",
    "        save a copy of the model.\n",
    "        \n",
    "        Args:\n",
    "          epochs: Number of training batches to use.\n",
    "          val_epochs: Number of validation batches to use each time validation\n",
    "            is performed.\n",
    "          val_period: Number of epochs to train for in between each occurrence\n",
    "            of validation (default 500).\n",
    "          save_period: Number of epochs to train for before saving another copy\n",
    "            of the model (default None, meaning that the model is not saved).\n",
    "        \"\"\"\n",
    "        # Note, this scheduler should not be used if one plans to call\n",
    "        # train_eval_loop multiple times.\n",
    "        \n",
    "        # It doesn't make sense to have more validation steps than batches in\n",
    "        # the validation set\n",
    "        val_epochs = min(val_epochs, len(self.val_loader))\n",
    "        # estimate total epochs\n",
    "        total_epochs = epochs + ((epochs // val_period) * val_epochs)\n",
    "        pbar = tqdm(total=total_epochs, desc=\"Training\")\n",
    "        \n",
    "        # Initialize iterator for validation set -- used to continue validation\n",
    "        # loop from where it left off\n",
    "        val_iterator = iter(cycle(self.val_loader))\n",
    "        \n",
    "        self.model.train()\n",
    "        total_train_loss = 0\n",
    "        for i, (X, y) in enumerate(cycle(self.train_loader)):\n",
    "            pbar.update()\n",
    "            if i >= epochs:\n",
    "                break\n",
    "            outputs, loss = self.train_step(X, y)\n",
    "            total_train_loss += loss.item()\n",
    "            if (i + 1) % val_period == 0:\n",
    "                # record number of epochs and training metrics\n",
    "                self.histories['epochs'].append(i+1)\n",
    "                self.histories['train_loss'].append(total_train_loss / val_period)\n",
    "\n",
    "                # record learning rate\n",
    "                self.histories['lr'].append(self.get_last_lr())\n",
    "\n",
    "                # predict on validation data and record metrics\n",
    "                self.model.eval()\n",
    "                total_val_loss = 0\n",
    "                for j, (val_X, val_y) in enumerate(val_iterator):\n",
    "                    pbar.update()\n",
    "                    if j >= val_epochs:\n",
    "                        break\n",
    "                    val_outputs, val_loss = self.val_step(val_X, val_y)\n",
    "                    total_val_loss += val_loss.item()\n",
    "                self.model.train()\n",
    "                \n",
    "                self.histories['val_loss'].append(total_val_loss / j)\n",
    "\n",
    "                # If logging to TensorBoard, add metrics to writer\n",
    "                if self.writer is not None:\n",
    "                    self.writer.add_scalars(\n",
    "                        main_tag=\"Loss\",\n",
    "                        tag_scalar_dict={\n",
    "                            \"train_loss\": self.histories['train_loss'][-1], \n",
    "                            \"val_loss\": self.histories['val_loss'][-1], \n",
    "                        }, \n",
    "                        global_step=i+1)\n",
    "                    self.writer.add_scalar(\n",
    "                        \"Learning rate\",\n",
    "                        self.histories['lr'][-1], \n",
    "                        global_step=i+1)\n",
    "                    # write to disk\n",
    "                    self.writer.flush()\n",
    "                    \n",
    "                # If loss is NaN, the model died and we might as well stop training.\n",
    "                if np.isnan(self.histories['val_loss'][-1]) or np.isnan(self.histories['train_loss'][-1]):\n",
    "                    print (f\"Model died at training epoch {i+1}, stopping training.\")\n",
    "                    break\n",
    "                    \n",
    "            # Optionally save a copy of the model\n",
    "            if save_period is not None and (i + 1) % save_period == 0:\n",
    "                self.save_checkpoint(f\"{i+1}_epochs\")\n",
    "                \n",
    "    def train_loop_simple(self, epochs):\n",
    "        \"\"\"Train model for a given number of epochs.\n",
    "        \n",
    "        I made this to diagnose the memory issues. It's an extremely simple\n",
    "        version of the training loop, without any extra functionality.\n",
    "        \n",
    "        Args:\n",
    "          epochs: Number of training batches to use.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        for i, (X, y) in enumerate(cycle(self.train_loader)):\n",
    "            if i >= epochs:\n",
    "                break\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X.to(DEVICE))\n",
    "            loss = self.criterion(outputs, y.to(DEVICE))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            if self.scheduler_class is not None:\n",
    "                self.scheduler.step()\n",
    "    \n",
    "    def time_train_step(self, n=500):\n",
    "        \"\"\"Time training on a given number of training batches. Note that this\n",
    "        does train the model.\n",
    "        \"\"\"\n",
    "        tic = time.perf_counter()\n",
    "        self.model.train()\n",
    "        for i in range(n):\n",
    "            X, y = next(iter(self.train_loader))\n",
    "            self.train_step(X, y)\n",
    "        toc = time.perf_counter()\n",
    "        delta = toc - tic\n",
    "        print(f\"Trained on {n} batches in {delta:.2f}s.\")\n",
    "        return n\n",
    "        \n",
    "    def time_val_step(self, n=500):\n",
    "        \"\"\"Time prediction on a given number of validation batches.\"\"\"\n",
    "        tic = time.perf_counter()\n",
    "        self.model.eval()\n",
    "        for i in range(n):\n",
    "            X, y = next(iter(self.val_loader))\n",
    "            self.val_step(X, y)\n",
    "        toc = time.perf_counter()\n",
    "        delta = toc - tic\n",
    "        print(f\"Predicted on {n} validation batches in {delta:.2f}s.\")\n",
    "        return n\n",
    "\n",
    "    def plot_metrics(self, ax=None):\n",
    "        \"\"\"Plot train and validation loss over time.\n",
    "        \n",
    "        Args:\n",
    "          ax: Axes to plot on (default plt.gca()).\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        plt.plot(self.histories['epochs'], self.histories['train_loss'], label=\"training\")\n",
    "        plt.plot(self.histories['epochs'], self.histories['val_loss'], label=\"validation\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.title(\"Loss\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "    def save_checkpoint(self, extra):\n",
    "        \"\"\"Save a copy of the model.\n",
    "        \n",
    "        The copy will be saved to MODEL_SAVE_DIR/{model_name}_{extra}.pt.\n",
    "        \n",
    "        Args:\n",
    "          extra: String to append to model name to generate filename for saving.\n",
    "        \"\"\"\n",
    "        model_save_path = MODEL_SAVE_DIR / (self.model_name + \"_\" + extra + \".pt\")\n",
    "        torch.save(self.model.state_dict(), model_save_path)\n",
    "        print(f\"Saved a checkpoint at {model_save_path}.\")\n",
    "\n",
    "\n",
    "### From models.py ###\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "class Spectrogram_EfficientNet(nn.Module):\n",
    "    \"\"\"An EfficientNetB0 vision model for the spectrogram data.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.efficientnet = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
    "        # freeze pretrained layers besides classifier\n",
    "        for param in self.efficientnet.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        # replace classifier with one of appropriate shape\n",
    "        self.efficientnet.classifier = nn.Linear(1280, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Convert grayscale images, [batch, H, W], to RGB images, [batch, 3, H, W]\n",
    "        return self.efficientnet(x.unsqueeze(1).repeat(1, 3, 1, 1))\n",
    "\n",
    "\n",
    "### From data_handling.py ###\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# BASE_PATH = Path(\"/kaggle/input/hms-harmful-brain-activity-classification\")\n",
    "BASE_PATH = Path(\"data/\")\n",
    "# EEG_DIR = Path(\"/tmp/dataset/hms-hbac\")\n",
    "EEG_DIR = Path(\"data/eeg_npy\")\n",
    "\n",
    "class_names = ['Seizure', 'LPD', 'GPD', 'LRDA','GRDA', 'Other']\n",
    "label2name = dict(enumerate(class_names))\n",
    "name2label = {v:k for k, v in label2name.items()}\n",
    "\n",
    "\n",
    "def create_eeg_npy_dirs():\n",
    "    \"\"\"Create folders to save the eeg .npys.\"\"\"\n",
    "    os.makedirs(EEG_DIR/'train_eegs', exist_ok=True)\n",
    "    os.makedirs(EEG_DIR/'test_eegs', exist_ok=True)\n",
    "    print(f\"Created directories {EEG_DIR/'train_eegs'}, {EEG_DIR/'test_eegs'}\")\n",
    "\n",
    "def metadata_df(split=\"train\"):\n",
    "    \"\"\"Return a DataFrame with metadata for the train or test set.\"\"\"\n",
    "    if split not in [\"train\", \"test\"]:\n",
    "        raise ValueError('Expected split=\"train\" or split=\"test\"')\n",
    "    metadata = pd.read_csv(f'{BASE_PATH}/{split}.csv')\n",
    "    metadata['eeg_path'] = f'{BASE_PATH}/{split}_eegs/'+metadata['eeg_id'].astype(str)+'.parquet'\n",
    "    metadata['spec_path'] = f'{BASE_PATH}/{split}_spectrograms/'+metadata['spectrogram_id'].astype(str)+'.parquet'\n",
    "    metadata['spec_npy_path'] = f'{SPEC_DIR}/{split}_spectrograms/'+metadata['spectrogram_id'].astype(str)+'.npy'\n",
    "    if split == \"train\":\n",
    "        metadata['class_label'] = metadata.expert_consensus.map(name2label)\n",
    "    return metadata\n",
    "\n",
    "def process_eeg(eeg_id, split=\"train\"):\n",
    "    \"\"\"Convert a single eeg parquet to .npy, and save the result.\"\"\"\n",
    "    eeg_path = f\"{BASE_PATH}/{split}_eegs/{eeg_id}.parquet\"\n",
    "    eeg = pd.read_parquet(eeg_path)\n",
    "    eeg = eeg.fillna(0).values[:, 1:].T # fill NaN values with 0, transpose for (Time, Freq) -> (Freq, Time)\n",
    "    eeg = eeg.astype(\"float32\")\n",
    "    np.save(f\"{EEG_DIR}/{split}_eegs/{eeg_id}.npy\", eeg)\n",
    "\n",
    "def process_all_eegs():\n",
    "    \"\"\"Convert and save all eegs. This could be slow.\"\"\"\n",
    "    # Get unique eeg_ids of train and valid data\n",
    "    metadata_train = metadata_df(\"train\")\n",
    "    eeg_ids = metadata_train[\"eeg_id\"].unique()\n",
    "\n",
    "    # Parallelize the processing using joblib for training data\n",
    "    _ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        joblib.delayed(process_eeg)(eeg_id, \"train\")\n",
    "        for eeg_id in tqdm(eeg_ids, total=len(eeg_ids))\n",
    "    )\n",
    "\n",
    "    # Get unique eeg_ids of test data\n",
    "    metadata_test = metadata_df(\"test\")\n",
    "    test_eeg_ids = metadata_test[\"eeg_id\"].unique()\n",
    "\n",
    "    # Parallelize the processing using joblib for test data\n",
    "    _ = joblib.Parallel(n_jobs=-1, backend=\"loky\")(\n",
    "        joblib.delayed(process_eeg)(eeg_id, \"test\")\n",
    "        for eeg_id in tqdm(test_eeg_ids, total=len(test_eeg_ids))\n",
    "    )\n",
    "    print(f\"Saved eegs as .npy files in {EEG_DIR}\")\n",
    "    \n",
    "class EegDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, metadata_df, item_transforms=None):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.item_transforms = item_transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        npy_path = self.metadata_df[\"eeg_npy_path\"].iloc[i]\n",
    "        offset = 200*int(self.metadata_df[\"eeg_label_offset_seconds\"].iloc[i])\n",
    "        #CHANGE THIS LINE PROPERLY\n",
    "        tens = torch.from_numpy(np.load(npy_path))[ offset:offset+10000]\n",
    "        if self.item_transforms is not None:\n",
    "            tens = self.item_transforms(tens)\n",
    "        expert_votes = self.metadata_df[[\n",
    "            \"seizure_vote\", \"lpd_vote\", \"gpd_vote\",\n",
    "            \"lrda_vote\", \"grda_vote\", \"other_vote\"\n",
    "        ]].iloc[i]\n",
    "        # target should be float so that nn.KLDivLoss works\n",
    "        target = torch.tensor(np.asarray(expert_votes)).float()\n",
    "        return tens, target\n",
    "    \n",
    "class EegTestDataset(EegDataset):\n",
    "    def __getitem__(self, i):\n",
    "        npy_path = self.metadata_df[\"eeg_npy_path\"].iloc[i]\n",
    "        tens = torch.from_numpy(np.load(npy_path))\n",
    "        target = None\n",
    "        return tens, target\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "### From logger.py ###\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def create_writer(model_name: str) -> SummaryWriter:\n",
    "    \"\"\"Create a SummaryWriter instance saving to a specific log_dir.\n",
    "    \n",
    "    This allows us to save metric histories, predictions, etc., to TensorBoard.\n",
    "    log_dir is formatted as logs/YYYY-MM-DD/model_name.\n",
    "    \n",
    "    Args:\n",
    "      model_name: Name of model.\n",
    "    \n",
    "    Returns:\n",
    "      A SummaryWriter object saving to log_dir.\n",
    "    \"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    log_dir = Path(\"logs\") / timestamp / model_name\n",
    "    print(f\"Created SummaryWriter saving to {log_dir}.\")\n",
    "    return SummaryWriter(log_dir=log_dir)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
